{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Evaluation Metrics for Classification.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "Bd2LjBZqQTDl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Evaluation Metrics for Classification\n",
        "\n",
        "Only the accuracy score for a classification model gives an incomplete picture of your modelâ€™s performance. The following evaluation metrics should help gain perspective of the practical usability of a classifier model.\n",
        "\n",
        "## Reference\n",
        "* https://towardsdatascience.com/evaluation-metrics-for-classification-409568938a7d\n",
        "* http://www.acheronanalytics.com/acheron-blog/how-do-machines-learn-bias-data-science\n",
        "* http://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\n",
        "* "
      ]
    },
    {
      "metadata": {
        "id": "nNN-qYDrKdY_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data generation"
      ]
    },
    {
      "metadata": {
        "id": "LlvvTwKFQSPE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "outputId": "fa9f0e36-332d-4caa-b80a-d7b55d94b89b"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.calibration import calibration_curve\n",
        "from scipy.stats import itemfreq\n",
        "\n",
        "np.random.seed(666)\n",
        "\n",
        "# Total sample size ------------------------------------------------------------\n",
        "n = 100000\n",
        "n_features = 20 # Total number of variables (features)\n",
        "n_informative = 2 # The number of informative variables (features).\n",
        "n_redundant = 2 # The number of redundant variables (features).\n",
        "# Samples used for training the models\n",
        "train_samples = 70000 \n",
        "\n",
        "X, y = datasets.make_classification(n_samples=n, \n",
        "                                    n_features=n_features,\n",
        "                                    n_informative=n_informative, \n",
        "                                    n_redundant=n_redundant)\n",
        "X = ( X * 5 ) + 50\n",
        "\n",
        "X_train = X[:train_samples]\n",
        "X_test = X[train_samples:]\n",
        "y_train = y[:train_samples]\n",
        "y_test = y[train_samples:]\n",
        "\n",
        "print(\"Shape of X_train:\", X_train.shape )\n",
        "print(\"Shape of y_train:\", y_train.shape, \"\\n\" )\n",
        "print(\"Shape of X_test:\", X_test.shape )\n",
        "print(\"Shape of y_test:\", y_test.shape, \"\\n\" )\n",
        "\n",
        "# Sample data\n",
        "print(\"Sample y \", y_train[1:5])\n",
        "print(\"Sample x \\n\", X_train[1:5], \"\\n\" )\n",
        "# Frequencia\n",
        "print(\"Frequency \\n\", itemfreq( y )) \n",
        "\n"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of X_train: (70000, 20)\n",
            "Shape of y_train: (70000,) \n",
            "\n",
            "Shape of X_test: (30000, 20)\n",
            "Shape of y_test: (30000,) \n",
            "\n",
            "Sample y  [1 1 1 0]\n",
            "Sample x \n",
            " [[64.94595194 53.13656135 53.01191613 56.21747762 55.32043575 44.5712013\n",
            "  48.25344799 45.74158019 50.52065627 47.40033855 41.84799332 49.32500722\n",
            "  43.73067361 43.02025042 50.78457529 43.09460704 44.32853147 50.86347644\n",
            "  52.21223543 42.09577081]\n",
            " [44.50695934 57.99924053 53.93509367 63.61508376 42.217573   45.33012077\n",
            "  53.09110498 46.79193816 42.99073914 51.0364739  54.01282072 50.82707778\n",
            "  51.57016636 44.67104363 45.8775338  44.6275318  53.37676574 42.39480428\n",
            "  47.71598573 55.71275738]\n",
            " [53.10043068 56.81066531 43.10342728 55.46104955 47.07898005 54.57684427\n",
            "  51.65586498 58.33450312 50.04754331 50.96330756 49.23096942 57.67504612\n",
            "  50.76146027 53.85844883 50.07657868 44.99553942 54.44174816 48.79662772\n",
            "  46.97121116 48.17265511]\n",
            " [47.0155751  48.3385013  38.87871553 41.00694044 51.2573889  49.39894809\n",
            "  48.60502111 51.23965781 56.54276302 47.16540401 47.93971954 56.83782732\n",
            "  57.49483569 53.19793283 49.63370927 57.00303054 46.92124077 58.37174886\n",
            "  53.54881558 46.46029504]] \n",
            "\n",
            "Frequency \n",
            " [[    0 49973]\n",
            " [    1 50027]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "An70d1kBKkNa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Define functions for Evaluaton Metrics for Classification\n",
        "\n",
        "This code defines Precision, Recell, F-measure, and Accuracy for Keras 2."
      ]
    },
    {
      "metadata": {
        "id": "QIMBWGgHj4qD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras import backend as K\n",
        "\n",
        "def mcor(y_true, y_pred):\n",
        "     #matthews_correlation\n",
        "     y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n",
        "     y_pred_neg = 1 - y_pred_pos\n",
        "  \n",
        "     y_pos = K.round(K.clip(y_true, 0, 1))\n",
        "     y_neg = 1 - y_pos\n",
        "  \n",
        "     tp = K.sum(y_pos * y_pred_pos)\n",
        "     tn = K.sum(y_neg * y_pred_neg)\n",
        "  \n",
        "     fp = K.sum(y_neg * y_pred_pos)\n",
        "     fn = K.sum(y_pos * y_pred_neg)\n",
        "  \n",
        "     numerator = (tp * tn - fp * fn)\n",
        "     denominator = K.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
        "  \n",
        "     return numerator / (denominator + K.epsilon())\n",
        "\n",
        "def precision(y_true, y_pred):\n",
        "    \"\"\"Precision metric.\n",
        "\n",
        "    Only computes a batch-wise average of precision.\n",
        "\n",
        "    Computes the precision, a metric for multi-label classification of\n",
        "    how many selected items are relevant.\n",
        "    \"\"\"\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def recall(y_true, y_pred):\n",
        "    \"\"\"Recall metric.\n",
        "\n",
        "    Only computes a batch-wise average of recall.\n",
        "\n",
        "    Computes the recall, a metric for multi-label classification of\n",
        "    how many relevant items are selected.\n",
        "    \"\"\"\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def accuracy(y_true, y_pred):\n",
        "    \"\"\"\n",
        "        Accuracy metric.\n",
        "\n",
        "        Only computes a Accuracy.\n",
        "\n",
        "        Computes true positives, true negatives, false positives\n",
        "        and false negatives.\n",
        "        \n",
        "        Accuracy = TP+TN/TP+FP+FN+TN\n",
        "        \n",
        "        REF: https://tryolabs.com/blog/2013/03/25/why-accuracy-alone-bad-measure-classification-tasks-and-what-we-can-do-about-it/\n",
        "    \"\"\"\n",
        "    y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n",
        "    y_pred_neg = 1 - y_pred_pos\n",
        " \n",
        " \n",
        "    y_pos = K.round(K.clip(y_true, 0, 1))\n",
        "    y_neg = 1 - y_pos\n",
        " \n",
        " \n",
        "    tp = K.sum(y_pos * y_pred_pos)\n",
        "    tn = K.sum(y_neg * y_pred_neg)\n",
        " \n",
        " \n",
        "    fp = K.sum(y_neg * y_pred_pos)\n",
        "    fn = K.sum(y_pos * y_pred_neg)\n",
        "    \n",
        "    accuracy = (tp+tn)/(tp+fp+fn+tn)\n",
        "    return(accuracy)\n",
        "  \n",
        "def f1(y_true, y_pred):\n",
        "    def recall(y_true, y_pred):\n",
        "        \"\"\"Recall metric.\n",
        "\n",
        "        Only computes a batch-wise average of recall.\n",
        "\n",
        "        Computes the recall, a metric for multi-label classification of\n",
        "        how many relevant items are selected.\n",
        "        \"\"\"\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "        recall = true_positives / (possible_positives + K.epsilon())\n",
        "        return recall\n",
        "\n",
        "    def precision(y_true, y_pred):\n",
        "        \"\"\"Precision metric.\n",
        "\n",
        "        Only computes a batch-wise average of precision.\n",
        "\n",
        "        Computes the precision, a metric for multi-label classification of\n",
        "        how many selected items are relevant.\n",
        "        \"\"\"\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "        precision = true_positives / (predicted_positives + K.epsilon())\n",
        "        return precision\n",
        "    precision = precision(y_true, y_pred)\n",
        "    recall = recall(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall))\n",
        "\n",
        "# you can use it like this\n",
        "# model.compile(loss='binary_crossentropy',\n",
        "#               optimizer= \"adam\",\n",
        "#               metrics=[mcor,recall, f1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uokDrHxNK8Jf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Model 1\n",
        "Model shows how to use, in Keras 2, custom defined functions ```metrics=[accuracy,precision,recall])``` . These functions are no more provided in Keras 2 by default."
      ]
    },
    {
      "metadata": {
        "id": "qKXd1YxTZRrP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 751
        },
        "outputId": "af3ef46c-3359-4327-8ce5-5698c96a1c98"
      },
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(64, input_dim=20, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=[accuracy,precision,recall])\n",
        "\n",
        "model.fit(X_train, y_train,\n",
        "          epochs=20,\n",
        "          batch_size=128)\n",
        "\n",
        "score = model.evaluate(X_test, y_test, verbose=0, batch_size=128)\n",
        "print('Test score:', score[0])\n",
        "print('Test accuracy:', score[1])\n",
        "\n",
        "# Prediction \n",
        "y_pred = model.predict(X_test)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "70000/70000 [==============================] - 5s 74us/step - loss: 1.1029 - accuracy: 0.7080 - precision: 0.7072 - recall: 0.7148\n",
            "Epoch 2/20\n",
            "70000/70000 [==============================] - 5s 66us/step - loss: 0.4401 - accuracy: 0.8026 - precision: 0.8183 - recall: 0.7850\n",
            "Epoch 3/20\n",
            "70000/70000 [==============================] - 5s 66us/step - loss: 0.4004 - accuracy: 0.8261 - precision: 0.8443 - recall: 0.8040\n",
            "Epoch 4/20\n",
            "38400/70000 [===============>..............] - ETA: 2s - loss: 0.3791 - accuracy: 0.8386 - precision: 0.8571 - recall: 0.8160"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "70000/70000 [==============================] - 5s 66us/step - loss: 0.3792 - accuracy: 0.8390 - precision: 0.8589 - recall: 0.8153\n",
            "Epoch 5/20\n",
            "70000/70000 [==============================] - 5s 66us/step - loss: 0.3632 - accuracy: 0.8470 - precision: 0.8688 - recall: 0.8215\n",
            "Epoch 6/20\n",
            "70000/70000 [==============================] - 5s 67us/step - loss: 0.3500 - accuracy: 0.8538 - precision: 0.8760 - recall: 0.8277\n",
            "Epoch 7/20\n",
            "51584/70000 [=====================>........] - ETA: 1s - loss: 0.3387 - accuracy: 0.8606 - precision: 0.8809 - recall: 0.8363"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "70000/70000 [==============================] - 5s 66us/step - loss: 0.3390 - accuracy: 0.8602 - precision: 0.8813 - recall: 0.8349\n",
            "Epoch 8/20\n",
            "70000/70000 [==============================] - 5s 66us/step - loss: 0.3309 - accuracy: 0.8635 - precision: 0.8848 - recall: 0.8388\n",
            "Epoch 9/20\n",
            "70000/70000 [==============================] - 5s 66us/step - loss: 0.3243 - accuracy: 0.8687 - precision: 0.8887 - recall: 0.8455\n",
            "Epoch 10/20\n",
            "55808/70000 [======================>.......] - ETA: 0s - loss: 0.3217 - accuracy: 0.8713 - precision: 0.8885 - recall: 0.8498"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "70000/70000 [==============================] - 5s 66us/step - loss: 0.3197 - accuracy: 0.8721 - precision: 0.8899 - recall: 0.8510\n",
            "Epoch 11/20\n",
            "70000/70000 [==============================] - 5s 66us/step - loss: 0.3131 - accuracy: 0.8743 - precision: 0.8907 - recall: 0.8551\n",
            "Epoch 12/20\n",
            "70000/70000 [==============================] - 5s 65us/step - loss: 0.3093 - accuracy: 0.8779 - precision: 0.8982 - recall: 0.8548\n",
            "Epoch 13/20\n",
            "56832/70000 [=======================>......] - ETA: 0s - loss: 0.3035 - accuracy: 0.8800 - precision: 0.8960 - recall: 0.8620"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "70000/70000 [==============================] - 5s 66us/step - loss: 0.3049 - accuracy: 0.8797 - precision: 0.8963 - recall: 0.8612\n",
            "Epoch 14/20\n",
            "70000/70000 [==============================] - 5s 65us/step - loss: 0.2989 - accuracy: 0.8832 - precision: 0.8995 - recall: 0.8645\n",
            "Epoch 15/20\n",
            "70000/70000 [==============================] - 5s 67us/step - loss: 0.2950 - accuracy: 0.8854 - precision: 0.9026 - recall: 0.8659\n",
            "Epoch 16/20\n",
            "52736/70000 [=====================>........] - ETA: 1s - loss: 0.2950 - accuracy: 0.8854 - precision: 0.8989 - recall: 0.8702"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "70000/70000 [==============================] - 5s 67us/step - loss: 0.2954 - accuracy: 0.8850 - precision: 0.8989 - recall: 0.8696\n",
            "Epoch 17/20\n",
            "70000/70000 [==============================] - 5s 66us/step - loss: 0.2908 - accuracy: 0.8873 - precision: 0.9012 - recall: 0.8711\n",
            "Epoch 18/20\n",
            "70000/70000 [==============================] - 5s 66us/step - loss: 0.2879 - accuracy: 0.8883 - precision: 0.8997 - recall: 0.8762\n",
            "Epoch 19/20\n",
            "56064/70000 [=======================>......] - ETA: 0s - loss: 0.2868 - accuracy: 0.8884 - precision: 0.9017 - recall: 0.8733"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "70000/70000 [==============================] - 5s 66us/step - loss: 0.2863 - accuracy: 0.8888 - precision: 0.9025 - recall: 0.8733\n",
            "Epoch 20/20\n",
            "70000/70000 [==============================] - 4s 64us/step - loss: 0.2855 - accuracy: 0.8908 - precision: 0.9033 - recall: 0.8768\n",
            "Test score: 0.24956810023387274\n",
            "Test accuracy: 0.9088333333651225\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dVCGOxgkLe7Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Model 2\n",
        "This model uses ```metrics=['accuracy'])```, default accuracy measure of Keras 2."
      ]
    },
    {
      "metadata": {
        "id": "slzZwxNErgMA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        },
        "outputId": "be618b3a-a928-4000-bf5a-4ac963dc5c27"
      },
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(64, input_dim=20, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['accuracy'])\n",
        "model.fit(X_train, y_train,\n",
        "          epochs=20,\n",
        "          batch_size=128)\n",
        "\n",
        "score = model.evaluate(X_test, y_test, verbose=0, batch_size=128)\n",
        "print('Test score:', score[0])\n",
        "print('Test accuracy:', score[1])\n",
        "\n",
        "# Prediction \n",
        "y_pred = model.predict(X_test)\n"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "70000/70000 [==============================] - 5s 68us/step - loss: 1.3052 - acc: 0.6906\n",
            "Epoch 2/20\n",
            "70000/70000 [==============================] - 4s 61us/step - loss: 0.4423 - acc: 0.7996\n",
            "Epoch 3/20\n",
            "70000/70000 [==============================] - 4s 60us/step - loss: 0.3989 - acc: 0.8266\n",
            "Epoch 4/20\n",
            "70000/70000 [==============================] - 4s 60us/step - loss: 0.3742 - acc: 0.8413\n",
            "Epoch 5/20\n",
            "36352/70000 [==============>...............] - ETA: 2s - loss: 0.3598 - acc: 0.8479"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "70000/70000 [==============================] - 4s 61us/step - loss: 0.3590 - acc: 0.8485\n",
            "Epoch 6/20\n",
            "70000/70000 [==============================] - 4s 60us/step - loss: 0.3480 - acc: 0.8561\n",
            "Epoch 7/20\n",
            "70000/70000 [==============================] - 4s 60us/step - loss: 0.3378 - acc: 0.8619\n",
            "Epoch 8/20\n",
            "70000/70000 [==============================] - 4s 60us/step - loss: 0.3319 - acc: 0.8662\n",
            "Epoch 9/20\n",
            "51072/70000 [====================>.........] - ETA: 1s - loss: 0.3284 - acc: 0.8666"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "70000/70000 [==============================] - 4s 60us/step - loss: 0.3255 - acc: 0.8682\n",
            "Epoch 10/20\n",
            "70000/70000 [==============================] - 4s 60us/step - loss: 0.3190 - acc: 0.8728\n",
            "Epoch 11/20\n",
            "70000/70000 [==============================] - 4s 60us/step - loss: 0.3117 - acc: 0.8754\n",
            "Epoch 12/20\n",
            "70000/70000 [==============================] - 4s 60us/step - loss: 0.3079 - acc: 0.8779\n",
            "Epoch 13/20\n",
            "52224/70000 [=====================>........] - ETA: 1s - loss: 0.3044 - acc: 0.8791"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "70000/70000 [==============================] - 4s 60us/step - loss: 0.3029 - acc: 0.8804\n",
            "Epoch 14/20\n",
            "70000/70000 [==============================] - 4s 60us/step - loss: 0.3003 - acc: 0.8816\n",
            "Epoch 15/20\n",
            "70000/70000 [==============================] - 4s 60us/step - loss: 0.2968 - acc: 0.8833\n",
            "Epoch 16/20\n",
            "70000/70000 [==============================] - 4s 59us/step - loss: 0.2968 - acc: 0.8855\n",
            "Epoch 17/20\n",
            "52608/70000 [=====================>........] - ETA: 1s - loss: 0.2885 - acc: 0.8877"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "70000/70000 [==============================] - 4s 59us/step - loss: 0.2903 - acc: 0.8868\n",
            "Epoch 18/20\n",
            "70000/70000 [==============================] - 4s 58us/step - loss: 0.2892 - acc: 0.8880\n",
            "Epoch 19/20\n",
            "70000/70000 [==============================] - 4s 63us/step - loss: 0.2876 - acc: 0.8896\n",
            "Epoch 20/20\n",
            "70000/70000 [==============================] - 4s 61us/step - loss: 0.2861 - acc: 0.8895\n",
            "Test score: 0.23711982057491937\n",
            "Test accuracy: 0.9077666666348775\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wnaR5IRQLpc1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Model 3\n",
        "This model implements the same previous model in a GPU."
      ]
    },
    {
      "metadata": {
        "id": "VQXJsPXsaZsC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1224
        },
        "outputId": "b4cca811-d97b-4571-b2cf-e51f3a14f824"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.random.seed(666)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.calibration import calibration_curve\n",
        "from scipy.stats import itemfreq\n",
        "\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "\n",
        "np.random.seed(666)\n",
        "\n",
        "with tf.device('/gpu:0'):\n",
        "  X, y = datasets.make_classification(n_samples=100000, n_features=20,\n",
        "                                      n_informative=2, n_redundant=2)\n",
        "\n",
        "  train_samples = 70000 # Samples used for training the models\n",
        "\n",
        "  X_train = X[:train_samples]\n",
        "  X_test = X[train_samples:]\n",
        "  y_train = y[:train_samples]\n",
        "  y_test = y[train_samples:]\n",
        "\n",
        "  print(\"Shape of X_train:\", X_train.shape )\n",
        "  print(\"Shape of y_train:\", y_train.shape, \"\\n\" )\n",
        "  print(\"Shape of X_test:\", X_test.shape )\n",
        "  print(\"Shape of y_test:\", y_test.shape, \"\\n\" )\n",
        "\n",
        "  # Sample data\n",
        "  print(\"Sample y \", y_train[1:5])\n",
        "  print(\"Sample x \\n\", X_train[1:5], \"\\n\" )\n",
        "  # Frequencia\n",
        "  print(\"Frequency \\n\", itemfreq( y )) \n",
        "    \n",
        "  model = Sequential()\n",
        "  model.add(Dense(64, input_dim=20, activation='relu'))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(64, activation='relu'))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "  model.compile(loss='binary_crossentropy',\n",
        "                optimizer='rmsprop',\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "  model.fit(X_train, y_train,\n",
        "            epochs=20,\n",
        "            batch_size=128)\n",
        "  score = model.evaluate(X_test, y_test, batch_size=128)\n",
        "  print(score)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n",
            "Shape of X_train: (70000, 20)\n",
            "Shape of y_train: (70000,) \n",
            "\n",
            "Shape of X_test: (30000, 20)\n",
            "Shape of y_test: (30000,) \n",
            "\n",
            "Sample y  [1 1 1 0]\n",
            "Sample x \n",
            " [[ 2.98919039  0.62731227  0.60238323  1.24349552  1.06408715 -1.08575974\n",
            "  -0.3493104  -0.85168396  0.10413125 -0.51993229 -1.63040134 -0.13499856\n",
            "  -1.25386528 -1.39594992  0.15691506 -1.38107859 -1.13429371  0.17269529\n",
            "   0.44244709 -1.58084584]\n",
            " [-1.09860813  1.59984811  0.78701873  2.72301675 -1.5564854  -0.93397585\n",
            "   0.618221   -0.64161237 -1.40185217  0.20729478  0.80256414  0.16541556\n",
            "   0.31403327 -1.06579127 -0.82449324 -1.07449364  0.67535315 -1.52103914\n",
            "  -0.45680285  1.14255148]\n",
            " [ 0.62008614  1.36213306 -1.37931454  1.09220991 -0.58420399  0.91536885\n",
            "   0.331173    1.66690062  0.00950866  0.19266151 -0.15380612  1.53500922\n",
            "   0.15229205  0.77168977  0.01531574 -1.00089212  0.88834963 -0.24067446\n",
            "  -0.60575777 -0.36546898]\n",
            " [-0.59688498 -0.33229974 -2.22425689 -1.79861191  0.25147778 -0.12021038\n",
            "  -0.27899578  0.24793156  1.3085526  -0.5669192  -0.41205609  1.36756546\n",
            "   1.49896714  0.63958657 -0.07325815  1.40060611 -0.61575185  1.67434977\n",
            "   0.70976312 -0.70794099]] \n",
            "\n",
            "Frequency \n",
            " [[    0 49973]\n",
            " [    1 50027]]\n",
            "Epoch 1/20\n",
            "64128/70000 [==========================>...] - ETA: 0s - loss: 0.3425 - acc: 0.8547"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "70000/70000 [==============================] - 5s 71us/step - loss: 0.3366 - acc: 0.8582\n",
            "Epoch 2/20\n",
            "70000/70000 [==============================] - 4s 61us/step - loss: 0.2587 - acc: 0.9033\n",
            "Epoch 3/20\n",
            "70000/70000 [==============================] - 4s 61us/step - loss: 0.2428 - acc: 0.9090\n",
            "Epoch 4/20\n",
            "70000/70000 [==============================] - 4s 61us/step - loss: 0.2367 - acc: 0.9113\n",
            "Epoch 5/20\n",
            "52480/70000 [=====================>........] - ETA: 1s - loss: 0.2343 - acc: 0.9122"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "70000/70000 [==============================] - 4s 62us/step - loss: 0.2341 - acc: 0.9122\n",
            "Epoch 6/20\n",
            "70000/70000 [==============================] - 4s 61us/step - loss: 0.2330 - acc: 0.9125\n",
            "Epoch 7/20\n",
            "70000/70000 [==============================] - 4s 60us/step - loss: 0.2308 - acc: 0.9128\n",
            "Epoch 8/20\n",
            "70000/70000 [==============================] - 4s 61us/step - loss: 0.2292 - acc: 0.9135\n",
            "Epoch 9/20\n",
            "52608/70000 [=====================>........] - ETA: 1s - loss: 0.2321 - acc: 0.9125"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "70000/70000 [==============================] - 4s 61us/step - loss: 0.2301 - acc: 0.9132\n",
            "Epoch 10/20\n",
            "70000/70000 [==============================] - 4s 61us/step - loss: 0.2299 - acc: 0.9135\n",
            "Epoch 11/20\n",
            "70000/70000 [==============================] - 4s 61us/step - loss: 0.2290 - acc: 0.9142\n",
            "Epoch 12/20\n",
            "70000/70000 [==============================] - 4s 60us/step - loss: 0.2293 - acc: 0.9140\n",
            "Epoch 13/20\n",
            "50816/70000 [====================>.........] - ETA: 1s - loss: 0.2305 - acc: 0.9135"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "70000/70000 [==============================] - 4s 61us/step - loss: 0.2292 - acc: 0.9140\n",
            "Epoch 14/20\n",
            "70000/70000 [==============================] - 4s 60us/step - loss: 0.2285 - acc: 0.9137\n",
            "Epoch 15/20\n",
            "70000/70000 [==============================] - 4s 61us/step - loss: 0.2287 - acc: 0.9143\n",
            "Epoch 16/20\n",
            "70000/70000 [==============================] - 4s 61us/step - loss: 0.2284 - acc: 0.9139\n",
            "Epoch 17/20\n",
            "49920/70000 [====================>.........] - ETA: 1s - loss: 0.2276 - acc: 0.9135"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "70000/70000 [==============================] - 4s 62us/step - loss: 0.2274 - acc: 0.9138\n",
            "Epoch 18/20\n",
            "70000/70000 [==============================] - 4s 60us/step - loss: 0.2277 - acc: 0.9138\n",
            "Epoch 19/20\n",
            "70000/70000 [==============================] - 4s 61us/step - loss: 0.2279 - acc: 0.9149\n",
            "Epoch 20/20\n",
            "70000/70000 [==============================] - 4s 61us/step - loss: 0.2276 - acc: 0.9144\n",
            "30000/30000 [==============================] - 1s 31us/step\n",
            "[0.21178311232328415, 0.9160666666984558]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6hJSToRdEfIp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Impact of Standardization\n",
        "The model below, is equivalent to model 1. However, all data (predictors) are standardized.\n",
        "\n",
        "REFERENCE: http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html"
      ]
    },
    {
      "metadata": {
        "id": "xfjjlyAAE3KD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        },
        "outputId": "4f3156ee-3023-41db-9573-6eb810dfd44a"
      },
      "cell_type": "code",
      "source": [
        "#mean_X = mean(X)\n",
        "#mean_Y = mean(y)\n",
        "#sd_X = sd(X)\n",
        "#sd_Y = sd(Y)\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Standardization of X\n",
        "std = StandardScaler(copy=True, with_mean=True, with_std=True)\n",
        "std = std.fit(X)\n",
        "X_std = std.transform(X)\n",
        "print(X_std[1:5])\n",
        "\n",
        "\n",
        "X_train = X_std[:train_samples]\n",
        "X_test = X_std[train_samples:]\n",
        "y_train = y[:train_samples]\n",
        "y_test = y[train_samples:]\n",
        "\n",
        "print(\"Shape of X_train:\", X_train.shape )\n",
        "print(\"Shape of y_train:\", y_train.shape, \"\\n\" )\n",
        "print(\"Shape of X_test:\", X_test.shape )\n",
        "print(\"Shape of y_test:\", y_test.shape, \"\\n\" )\n",
        "\n",
        "# Sample data\n",
        "print(\"Sample y \", y_train[1:5])\n",
        "print(\"Sample x \\n\", X_train[1:5], \"\\n\" )\n",
        "# Frequencia\n",
        "print(\"Frequency \\n\", itemfreq( y )) "
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 2.98996918  0.61800881  0.46626613  0.95613576  1.06147415 -1.08968617\n",
            "  -0.35555945 -0.84174552  0.10518837 -0.51963654 -1.63628803 -0.11655378\n",
            "  -1.2497653  -1.40117807  0.1601449  -1.39100283 -1.13605005  0.17237447\n",
            "   0.44075883 -1.57697059]\n",
            " [-1.09537199  1.57270675  0.60947031  2.09153589 -1.55498828 -0.93762498\n",
            "   0.61049051 -0.63242725 -1.39941584  0.20632752  0.80075038  0.1463433\n",
            "   0.31511071 -1.07008951 -0.82063344 -1.08313307  0.67735211 -1.52142631\n",
            "  -0.45994077  1.14330883]\n",
            " [ 0.62228909  1.33935179 -1.0707481   0.84003759 -0.5842318   0.91509836\n",
            "   0.32388202  1.6678073   0.01065244  0.19171967 -0.1572209   1.34489608\n",
            "   0.15368129  0.77256692  0.01863647 -1.00922311  0.89079059 -0.24101147\n",
            "  -0.60913581 -0.36298516]\n",
            " [-0.59395045 -0.32400234 -1.72608942 -1.37840945  0.25013928 -0.12237282\n",
            "  -0.28535248  0.25392673  1.30850669 -0.56654184 -0.4159032   1.19836344\n",
            "   1.49776031  0.64009164 -0.06988056  1.40233901 -0.61643214  1.67408777\n",
            "   0.7085058  -0.70506509]]\n",
            "Shape of X_train: (70000, 20)\n",
            "Shape of y_train: (70000,) \n",
            "\n",
            "Shape of X_test: (30000, 20)\n",
            "Shape of y_test: (30000,) \n",
            "\n",
            "Sample y  [1 1 1 0]\n",
            "Sample x \n",
            " [[ 2.98996918  0.61800881  0.46626613  0.95613576  1.06147415 -1.08968617\n",
            "  -0.35555945 -0.84174552  0.10518837 -0.51963654 -1.63628803 -0.11655378\n",
            "  -1.2497653  -1.40117807  0.1601449  -1.39100283 -1.13605005  0.17237447\n",
            "   0.44075883 -1.57697059]\n",
            " [-1.09537199  1.57270675  0.60947031  2.09153589 -1.55498828 -0.93762498\n",
            "   0.61049051 -0.63242725 -1.39941584  0.20632752  0.80075038  0.1463433\n",
            "   0.31511071 -1.07008951 -0.82063344 -1.08313307  0.67735211 -1.52142631\n",
            "  -0.45994077  1.14330883]\n",
            " [ 0.62228909  1.33935179 -1.0707481   0.84003759 -0.5842318   0.91509836\n",
            "   0.32388202  1.6678073   0.01065244  0.19171967 -0.1572209   1.34489608\n",
            "   0.15368129  0.77256692  0.01863647 -1.00922311  0.89079059 -0.24101147\n",
            "  -0.60913581 -0.36298516]\n",
            " [-0.59395045 -0.32400234 -1.72608942 -1.37840945  0.25013928 -0.12237282\n",
            "  -0.28535248  0.25392673  1.30850669 -0.56654184 -0.4159032   1.19836344\n",
            "   1.49776031  0.64009164 -0.06988056  1.40233901 -0.61643214  1.67408777\n",
            "   0.7085058  -0.70506509]] \n",
            "\n",
            "Frequency \n",
            " [[    0 49973]\n",
            " [    1 50027]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SxvWrJr2EdmL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 751
        },
        "outputId": "b1dbaff2-e23a-4e9b-a50d-2348f2f2d7f0"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(64, input_dim=20, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=[accuracy,precision,recall])\n",
        "\n",
        "model.fit(X_train, y_train,\n",
        "          epochs=20,\n",
        "          batch_size=128)\n",
        "\n",
        "score = model.evaluate(X_test, y_test, verbose=0, batch_size=128)\n",
        "print('Test score:', score[0])\n",
        "print('Test accuracy:', score[1])\n",
        "\n",
        "# Prediction \n",
        "y_pred = model.predict(X_test)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "70000/70000 [==============================] - 5s 78us/step - loss: 0.3444 - accuracy: 0.8537 - precision: 0.8629 - recall: 0.8428\n",
            "Epoch 2/20\n",
            "70000/70000 [==============================] - 5s 66us/step - loss: 0.2629 - accuracy: 0.9007 - precision: 0.9126 - recall: 0.8867\n",
            "Epoch 3/20\n",
            "70000/70000 [==============================] - 5s 66us/step - loss: 0.2497 - accuracy: 0.9068 - precision: 0.9184 - recall: 0.8932\n",
            "Epoch 4/20\n",
            "36992/70000 [==============>...............] - ETA: 2s - loss: 0.2447 - accuracy: 0.9068 - precision: 0.9200 - recall: 0.8910"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "70000/70000 [==============================] - 5s 68us/step - loss: 0.2402 - accuracy: 0.9088 - precision: 0.9221 - recall: 0.8932\n",
            "Epoch 5/20\n",
            "70000/70000 [==============================] - 5s 67us/step - loss: 0.2361 - accuracy: 0.9120 - precision: 0.9253 - recall: 0.8969\n",
            "Epoch 6/20\n",
            "70000/70000 [==============================] - 5s 66us/step - loss: 0.2330 - accuracy: 0.9123 - precision: 0.9246 - recall: 0.8983\n",
            "Epoch 7/20\n",
            "52480/70000 [=====================>........] - ETA: 1s - loss: 0.2315 - accuracy: 0.9138 - precision: 0.9261 - recall: 0.9002"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "70000/70000 [==============================] - 5s 66us/step - loss: 0.2321 - accuracy: 0.9135 - precision: 0.9249 - recall: 0.9005\n",
            "Epoch 8/20\n",
            "70000/70000 [==============================] - 5s 64us/step - loss: 0.2320 - accuracy: 0.9128 - precision: 0.9236 - recall: 0.9003\n",
            "Epoch 9/20\n",
            "70000/70000 [==============================] - 5s 65us/step - loss: 0.2311 - accuracy: 0.9130 - precision: 0.9243 - recall: 0.8999\n",
            "Epoch 10/20\n",
            "58624/70000 [========================>.....] - ETA: 0s - loss: 0.2300 - accuracy: 0.9134 - precision: 0.9241 - recall: 0.9010"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "70000/70000 [==============================] - 5s 66us/step - loss: 0.2303 - accuracy: 0.9133 - precision: 0.9240 - recall: 0.9009\n",
            "Epoch 11/20\n",
            "70000/70000 [==============================] - 5s 66us/step - loss: 0.2294 - accuracy: 0.9135 - precision: 0.9242 - recall: 0.9014\n",
            "Epoch 12/20\n",
            "70000/70000 [==============================] - 5s 66us/step - loss: 0.2291 - accuracy: 0.9134 - precision: 0.9243 - recall: 0.9010\n",
            "Epoch 13/20\n",
            "56832/70000 [=======================>......] - ETA: 0s - loss: 0.2282 - accuracy: 0.9138 - precision: 0.9247 - recall: 0.9008"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "70000/70000 [==============================] - 5s 66us/step - loss: 0.2277 - accuracy: 0.9143 - precision: 0.9252 - recall: 0.9016\n",
            "Epoch 14/20\n",
            "70000/70000 [==============================] - 5s 66us/step - loss: 0.2285 - accuracy: 0.9138 - precision: 0.9244 - recall: 0.9017\n",
            "Epoch 15/20\n",
            "70000/70000 [==============================] - 5s 65us/step - loss: 0.2281 - accuracy: 0.9139 - precision: 0.9242 - recall: 0.9017\n",
            "Epoch 16/20\n",
            "54400/70000 [======================>.......] - ETA: 1s - loss: 0.2292 - accuracy: 0.9150 - precision: 0.9245 - recall: 0.9037"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "70000/70000 [==============================] - 5s 67us/step - loss: 0.2297 - accuracy: 0.9147 - precision: 0.9245 - recall: 0.9034\n",
            "Epoch 17/20\n",
            "70000/70000 [==============================] - 5s 66us/step - loss: 0.2282 - accuracy: 0.9142 - precision: 0.9245 - recall: 0.9023\n",
            "Epoch 18/20\n",
            "70000/70000 [==============================] - 5s 66us/step - loss: 0.2272 - accuracy: 0.9143 - precision: 0.9237 - recall: 0.9036\n",
            "Epoch 19/20\n",
            "55168/70000 [======================>.......] - ETA: 0s - loss: 0.2281 - accuracy: 0.9147 - precision: 0.9246 - recall: 0.9037"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "70000/70000 [==============================] - 5s 66us/step - loss: 0.2290 - accuracy: 0.9138 - precision: 0.9238 - recall: 0.9022\n",
            "Epoch 20/20\n",
            "70000/70000 [==============================] - 5s 66us/step - loss: 0.2280 - accuracy: 0.9142 - precision: 0.9240 - recall: 0.9033\n",
            "Test score: 0.21285472310384115\n",
            "Test accuracy: 0.9161666666984558\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9nIiBUvNP8SF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Precision\n",
        "\n",
        "Precision is the percentage of correctly classified true positives as a percentage of the positive predictions. High precision means that you correctly label as many of the true positives as possible. For example, a medical diagnostic tool should be very precise because not catching an illness can cause an illness to worsen."
      ]
    },
    {
      "metadata": {
        "id": "jmp8m5DsQBEC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Recall\n",
        "\n",
        "Recall on the other hand is the percentage of relevant elements returned. For example, if you search for Harry Potter books on Google, recall will be the number of Harry Potter titles returned divided by seven.\n",
        "\n",
        "Ideally we will have a recall of 1. In this case, it might be a nuisance, and a terrible user experience to sift through irrelevant search results. Additionally, if a user does not see relevant results, they will likely not make any purchases, which eventually could hurt the bottom line. "
      ]
    },
    {
      "metadata": {
        "id": "r7OeJqK5QIHG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Accuracy\n",
        "\n",
        "Accuracy is a measure of all the correct predictions as a percentage of the total predictions. Accuracy does poorly as a measure of model performance especially where you have unbalanced classes.11\n",
        "For precision, recall, accuracy, and confusion matrices to make sense to begin with, the training data should be representative of the population such that the model learns how to classify correctly. "
      ]
    },
    {
      "metadata": {
        "id": "E3D1jSh8Q87Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## F-1 score\n",
        "\n",
        "The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0. The relative contribution of precision and recall to the F1 score are equal. The formula for the F1 score is:\n",
        "\n",
        "```\n",
        "F1 = 2 * (precision * recall) / (precision + recall)\n",
        "```\n",
        "In the multi-class and multi-label case, this is the weighted average of the F1 score of each class.\n"
      ]
    },
    {
      "metadata": {
        "id": "_70r7RzRqPYV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Extra code\n",
        "Interesting, but not important code. Development."
      ]
    },
    {
      "metadata": {
        "id": "Do9VnmtlhkK6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras import backend as K\n",
        "\n",
        "def matthews_correlation(y_true, y_pred):\n",
        "    \"\"\"Matthews correlation metric.\n",
        "# Aliases\n",
        "\n",
        "    It is only computed as a batch-wise average, not globally.\n",
        "\n",
        "    Computes the Matthews correlation coefficient measure for quality\n",
        "    of binary classification problems.\n",
        "    \"\"\"\n",
        "    y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n",
        "    y_pred_neg = 1 - y_pred_pos\n",
        "\n",
        "    y_pos = K.round(K.clip(y_true, 0, 1))\n",
        "    y_neg = 1 - y_pos\n",
        "\n",
        "    tp = K.sum(y_pos * y_pred_pos)\n",
        "    tn = K.sum(y_neg * y_pred_neg)\n",
        "\n",
        "    fp = K.sum(y_neg * y_pred_pos)\n",
        "    fn = K.sum(y_pos * y_pred_neg)\n",
        "\n",
        "    numerator = (tp * tn - fp * fn)\n",
        "    denominator = K.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
        "\n",
        "    return numerator / (denominator + K.epsilon())\n",
        "\n",
        "\n",
        "def precision(y_true, y_pred):\n",
        "    \"\"\"Precision metric.\n",
        "\n",
        "    Only computes a batch-wise average of precision.\n",
        "\n",
        "    Computes the precision, a metric for multi-label classification of\n",
        "    how many selected items are relevant.\n",
        "    \"\"\"\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "\n",
        "def recall(y_true, y_pred):\n",
        "    \"\"\"Recall metric.\n",
        "\n",
        "    Only computes a batch-wise average of recall.\n",
        "\n",
        "    Computes the recall, a metric for multi-label classification of\n",
        "    how many relevant items are selected.\n",
        "    \"\"\"\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "\n",
        "def fbeta_score(y_true, y_pred, beta=1):\n",
        "    \"\"\"Computes the F score.\n",
        "\n",
        "    The F score is the weighted harmonic mean of precision and recall.\n",
        "    Here it is only computed as a batch-wise average, not globally.\n",
        "\n",
        "    This is useful for multi-label classification, where input samples can be\n",
        "    classified as sets of labels. By only using accuracy (precision) a model\n",
        "    would achieve a perfect score by simply assigning every class to every\n",
        "    input. In order to avoid this, a metric should penalize incorrect class\n",
        "    assignments as well (recall). The F-beta score (ranged from 0.0 to 1.0)\n",
        "    computes this, as a weighted mean of the proportion of correct class\n",
        "    assignments vs. the proportion of incorrect class assignments.\n",
        "\n",
        "    With beta = 1, this is equivalent to a F-measure. With beta < 1, assigning\n",
        "    correct classes becomes more important, and with beta > 1 the metric is\n",
        "    instead weighted towards penalizing incorrect class assignments.\n",
        "    \"\"\"\n",
        "    if beta < 0:\n",
        "        raise ValueError('The lowest choosable beta is zero (only precision).')\n",
        "\n",
        "    # If there are no true positives, fix the F score at 0 like sklearn.\n",
        "    if K.sum(K.round(K.clip(y_true, 0, 1))) == 0:\n",
        "        return 0\n",
        "\n",
        "    p = precision(y_true, y_pred)\n",
        "    r = recall(y_true, y_pred)\n",
        "    bb = beta ** 2\n",
        "    fbeta_score = (1 + bb) * (p * r) / (bb * p + r + K.epsilon())\n",
        "    return fbeta_score\n",
        "\n",
        "\n",
        "def fmeasure(y_true, y_pred):\n",
        "    \"\"\"Computes the f-measure, the harmonic mean of precision and recall.\n",
        "\n",
        "    Here it is only computed as a batch-wise average, not globally.\n",
        "    \"\"\"\n",
        "    return fbeta_score(y_true, y_pred, beta=1)\n",
        "\n",
        "\n",
        "# aliases\n",
        "fscore = f1score = fmeasure"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pwjtMvtSYkN9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "np.random.seed(666)\n",
        "\n",
        "# Generate dummy data ----------------------------------------------------------\n",
        "n = 100000\n",
        "perc_train = 0.9\n",
        "perc_test = 1 - perc_train\n",
        "n_variables = 20 # number of features (variables)\n",
        "\n",
        "X_train = np.random.random((int(n*perc_train), 20))\n",
        "y_train = np.random.randint(2, size=(int(n*perc_train), 1))\n",
        "X_test = np.random.random((n - int(n*perc_train), 20))\n",
        "y_test = np.random.randint(2, size=(n - int(n*perc_train), 1))\n",
        "\n",
        "print(\"Shape of X_train:\", X_train.shape )\n",
        "print(\"Shape of y_train:\", y_train.shape, \"\\n\" )\n",
        "print(\"Shape of X_test:\", X_test.shape )\n",
        "print(\"Shape of y_test:\", y_test.shape, \"\\n\" )\n",
        "\n",
        "# Sample data\n",
        "print(\"Sample y \", y_train[1:5])\n",
        "print(\"Sample x \\n\", X_train[1:5], \"\\n\" )\n",
        "# Frequencia\n",
        "print(\"Frequency \\n\", itemfreq( y )) \n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}